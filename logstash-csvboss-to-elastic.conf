# This pipeline transfers api logs from csv files to postgres database
#./logstash -f "E:\logstash-configs\logstash-csvboss-to-elastic.conf"

input {
	file {
		path => "E:/Documents/IndAddresses2.csv"
		start_position => "beginning"
		sincedb_path => "nul"
	}
}

filter {
    # Add filter here. This sample has a blank filter.
	csv { 
		columns => ["AccountNumber","custom_building","custom_additionalnumber","custom_Zipcode","alixis_StreetNameAR","alixis_StreetNameEN","alixis_DistrictNameAR","alixis_DistrictNameEN","custom_CityName","custom_regionName","UnitType","custom_unitnumber","OccupationMode","Relationship","PIDType","fullName","alixis_FirstName","alixis_secondname","alixis_thirdname","alixis_LastName","alixis_FullNameEnglish","alixis_FirstNameEnglish","alixis_SecondNameEnglish","alixis_ThirdNameEnglish","alixis_LastNameEnglish","alixis_PID","alixis_PIDIssueDate","alixis_PIDExpiryDateHijri","alixis_idissueplace","alixis_birthdate_hijri","alixis_PlaceofBirth","alixis_NationalityName","gender","mobileNo","Email","Occupation","alixis_SponsorID","alixis_SponsorName","alixis_physicallychallenged","CreatedOn"] 
		separator => "," 
		skip_header => "true"
	}
}

output {
  elasticsearch { 
    hosts => "esproxy.address.gov.sa:80"
	index  => "boss-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}